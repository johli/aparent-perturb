{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2023-04-25 23:32:31.811408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "'''\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#import seaborn as sns\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import pickle\n",
    "from time import sleep\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import h5py\n",
    "\n",
    "#Visualization code\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "def ic_scale(pwm,background):\n",
    "    per_position_ic = util.compute_per_position_ic(\n",
    "                       ppm=pwm, background=background, pseudocount=0.001)\n",
    "    return pwm*(per_position_ic[:,None])\n",
    "\n",
    "\n",
    "def plot_a(ax, base, left_edge, height, color):\n",
    "    a_polygon_coords = [\n",
    "        np.array([\n",
    "           [0.0, 0.0],\n",
    "           [0.5, 1.0],\n",
    "           [0.5, 0.8],\n",
    "           [0.2, 0.0],\n",
    "        ]),\n",
    "        np.array([\n",
    "           [1.0, 0.0],\n",
    "           [0.5, 1.0],\n",
    "           [0.5, 0.8],\n",
    "           [0.8, 0.0],\n",
    "        ]),\n",
    "        np.array([\n",
    "           [0.225, 0.45],\n",
    "           [0.775, 0.45],\n",
    "           [0.85, 0.3],\n",
    "           [0.15, 0.3],\n",
    "        ])\n",
    "    ]\n",
    "    for polygon_coords in a_polygon_coords:\n",
    "        ax.add_patch(matplotlib.patches.Polygon((np.array([1,height])[None,:]*polygon_coords\n",
    "                                                 + np.array([left_edge,base])[None,:]),\n",
    "                                                facecolor=color, edgecolor=color))\n",
    "\n",
    "\n",
    "def plot_c(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=1.3, height=height,\n",
    "                                            facecolor=color, edgecolor=color))\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=0.7*1.3, height=0.7*height,\n",
    "                                            facecolor='white', edgecolor='white'))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+1, base], width=1.0, height=height,\n",
    "                                            facecolor='white', edgecolor='white', fill=True))\n",
    "\n",
    "\n",
    "def plot_g(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=1.3, height=height,\n",
    "                                            facecolor=color, edgecolor=color))\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=0.7*1.3, height=0.7*height,\n",
    "                                            facecolor='white', edgecolor='white'))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+1, base], width=1.0, height=height,\n",
    "                                            facecolor='white', edgecolor='white', fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.825, base+0.085*height], width=0.174, height=0.415*height,\n",
    "                                            facecolor=color, edgecolor=color, fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.625, base+0.35*height], width=0.374, height=0.15*height,\n",
    "                                            facecolor=color, edgecolor=color, fill=True))\n",
    "\n",
    "\n",
    "def plot_t(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.4, base],\n",
    "                  width=0.2, height=height, facecolor=color, edgecolor=color, fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge, base+0.8*height],\n",
    "                  width=1.0, height=0.2*height, facecolor=color, edgecolor=color, fill=True))\n",
    "\n",
    "default_colors = {0:'green', 1:'blue', 2:'orange', 3:'red'}\n",
    "default_plot_funcs = {0:plot_a, 1:plot_c, 2:plot_g, 3:plot_t}\n",
    "def plot_weights_given_ax(ax, array,\n",
    "                 figsize=(20,2),\n",
    "                 height_padding_factor=0.2,\n",
    "                 length_padding=1.0,\n",
    "                 subticks_frequency=1.0,\n",
    "                 colors=default_colors,\n",
    "                 plot_funcs=default_plot_funcs,\n",
    "                 highlight={},\n",
    "                 ylabel=\"\"):\n",
    "    if len(array.shape)==3:\n",
    "        array = np.squeeze(array)\n",
    "    assert len(array.shape)==2, array.shape\n",
    "    if (array.shape[0]==4 and array.shape[1] != 4):\n",
    "        array = array.transpose(1,0)\n",
    "    assert array.shape[1]==4\n",
    "    max_pos_height = 0.0\n",
    "    min_neg_height = 0.0\n",
    "    heights_at_positions = []\n",
    "    depths_at_positions = []\n",
    "    for i in range(array.shape[0]):\n",
    "        #sort from smallest to highest magnitude\n",
    "        acgt_vals = sorted(enumerate(array[i,:]), key=lambda x: abs(x[1]))\n",
    "        positive_height_so_far = 0.0\n",
    "        negative_height_so_far = 0.0\n",
    "        for letter in acgt_vals:\n",
    "            plot_func = plot_funcs[letter[0]]\n",
    "            color=colors[letter[0]]\n",
    "            if (letter[1] > 0):\n",
    "                height_so_far = positive_height_so_far\n",
    "                positive_height_so_far += letter[1]                \n",
    "            else:\n",
    "                height_so_far = negative_height_so_far\n",
    "                negative_height_so_far += letter[1]\n",
    "            plot_func(ax=ax, base=height_so_far, left_edge=i, height=letter[1], color=color)\n",
    "        max_pos_height = max(max_pos_height, positive_height_so_far)\n",
    "        min_neg_height = min(min_neg_height, negative_height_so_far)\n",
    "        heights_at_positions.append(positive_height_so_far)\n",
    "        depths_at_positions.append(negative_height_so_far)\n",
    "\n",
    "    #now highlight any desired positions; the key of\n",
    "    #the highlight dict should be the color\n",
    "    for color in highlight:\n",
    "        for start_pos, end_pos in highlight[color]:\n",
    "            assert start_pos >= 0.0 and end_pos <= array.shape[0]\n",
    "            min_depth = np.min(depths_at_positions[start_pos:end_pos])\n",
    "            max_height = np.max(heights_at_positions[start_pos:end_pos])\n",
    "            ax.add_patch(\n",
    "                matplotlib.patches.Rectangle(xy=[start_pos,min_depth],\n",
    "                    width=end_pos-start_pos,\n",
    "                    height=max_height-min_depth,\n",
    "                    edgecolor=color, fill=False))\n",
    "            \n",
    "    ax.set_xlim(-length_padding, array.shape[0]+length_padding)\n",
    "    ax.xaxis.set_ticks(np.arange(0.0, array.shape[0]+1, subticks_frequency))\n",
    "    height_padding = max(abs(min_neg_height)*(height_padding_factor),\n",
    "                         abs(max_pos_height)*(height_padding_factor))\n",
    "    ax.set_ylim(min_neg_height-height_padding, max_pos_height+height_padding)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.yaxis.label.set_fontsize(15)\n",
    "\n",
    "\n",
    "def plot_weights(array,\n",
    "                 figsize=(20,2),\n",
    "                 **kwargs):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111) \n",
    "    plot_weights_given_ax(ax=ax, array=array,**kwargs)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score_track_given_ax(arr, ax, threshold=None, **kwargs):\n",
    "    ax.plot(np.arange(len(arr)), arr, **kwargs)\n",
    "    if (threshold is not None):\n",
    "        ax.plot([0, len(arr)-1], [threshold, threshold])\n",
    "    ax.set_xlim(0,len(arr)-1)\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def plot_score_track(arr, threshold=None, figsize=(20,2), **kwargs):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111) \n",
    "    plot_score_track_given_ax(arr, threshold=threshold, ax=ax, **kwargs) \n",
    "    plt.show()\n",
    "\n",
    "def plot_pwm(weights, figsize=(16, 2), plot_y_ticks=True, y_min=None, y_max=None, save_figs=False, fig_name=\"default\") :\n",
    "    colors = {0:'green', 1:'blue', 2:'orange', 3:'red'}\n",
    "    \n",
    "    plot_funcs = {0: plot_a, 1: plot_c, \n",
    "                  2: plot_g, 3: plot_t}\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    ax = fig.add_subplot(111) \n",
    "    \n",
    "    plot_weights_given_ax(ax=ax, array=weights, \n",
    "                                       height_padding_factor=0.2,\n",
    "                                       length_padding=1.0, \n",
    "                                       subticks_frequency=1.0, \n",
    "                                       colors=colors, plot_funcs=plot_funcs, \n",
    "                                       highlight={}, ylabel=\"\")\n",
    "\n",
    "    plt.sca(ax)\n",
    "    \n",
    "    plt.xticks([], [])\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    \n",
    "    if plot_y_ticks :\n",
    "        plt.yticks(fontsize=12)\n",
    "    else :\n",
    "        plt.yticks([], [])\n",
    "    \n",
    "    if y_min is not None and y_max is not None :\n",
    "        plt.ylim(y_min, y_max)\n",
    "    elif y_min is not None :\n",
    "        plt.ylim(y_min)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figs :\n",
    "        plt.savefig(fig_name + \".png\", transparent=True, dpi=600)\n",
    "        plt.savefig(fig_name + \".eps\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.text import TextPath\n",
    "from matplotlib.patches import PathPatch, Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def dna_letter_at(letter, x, y, yscale=1, ax=None, color=None, alpha=1.0):\n",
    "\n",
    "    fp = FontProperties(family=\"DejaVu Sans\", weight=\"bold\")\n",
    "    globscale = 1.35\n",
    "    LETTERS = {\t\"T\" : TextPath((-0.305, 0), \"T\", size=1, prop=fp),\n",
    "                \"G\" : TextPath((-0.384, 0), \"G\", size=1, prop=fp),\n",
    "                \"A\" : TextPath((-0.35, 0), \"A\", size=1, prop=fp),\n",
    "                \"C\" : TextPath((-0.366, 0), \"C\", size=1, prop=fp),\n",
    "                \"UP\" : TextPath((-0.488, 0), '$\\\\Uparrow$', size=1, prop=fp),\n",
    "                \"DN\" : TextPath((-0.488, 0), '$\\\\Downarrow$', size=1, prop=fp),\n",
    "                \"(\" : TextPath((-0.25, 0), \"(\", size=1, prop=fp),\n",
    "                \".\" : TextPath((-0.125, 0), \"-\", size=1, prop=fp),\n",
    "                \")\" : TextPath((-0.1, 0), \")\", size=1, prop=fp)}\n",
    "    COLOR_SCHEME = {'G': 'orange',#'orange', \n",
    "                    'A': 'green',#'red', \n",
    "                    'C': 'blue',#'blue', \n",
    "                    'T': 'red',#'darkgreen',\n",
    "                    'UP': 'green', \n",
    "                    'DN': 'red',\n",
    "                    '(': 'black',\n",
    "                    '.': 'black', \n",
    "                    ')': 'black'}\n",
    "\n",
    "\n",
    "    text = LETTERS[letter]\n",
    "\n",
    "    chosen_color = COLOR_SCHEME[letter]\n",
    "    if color is not None :\n",
    "        chosen_color = color\n",
    "\n",
    "    t = mpl.transforms.Affine2D().scale(1*globscale, yscale*globscale) + \\\n",
    "        mpl.transforms.Affine2D().translate(x,y) + ax.transData\n",
    "    p = PathPatch(text, lw=0, fc=chosen_color, alpha=alpha, transform=t)\n",
    "    if ax != None:\n",
    "        ax.add_artist(p)\n",
    "    return p\n",
    "\n",
    "def plot_seq_scores(importance_scores, figsize=(16, 2), plot_y_ticks=True, y_min=None, y_max=None, save_figs=False, fig_name=\"default\") :\n",
    "\n",
    "    importance_scores = importance_scores.T\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    ref_seq = \"\"\n",
    "    for j in range(importance_scores.shape[1]) :\n",
    "        argmax_nt = np.argmax(np.abs(importance_scores[:, j]))\n",
    "        \n",
    "        if argmax_nt == 0 :\n",
    "            ref_seq += \"A\"\n",
    "        elif argmax_nt == 1 :\n",
    "            ref_seq += \"C\"\n",
    "        elif argmax_nt == 2 :\n",
    "            ref_seq += \"G\"\n",
    "        elif argmax_nt == 3 :\n",
    "            ref_seq += \"T\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for i in range(0, len(ref_seq)) :\n",
    "        mutability_score = np.sum(importance_scores[:, i])\n",
    "        color = None\n",
    "        dna_letter_at(ref_seq[i], i + 0.5, 0, mutability_score, ax, color=color)\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    plt.xticks([], [])\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    \n",
    "    plt.xlim((0, len(ref_seq)))\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    \n",
    "    if plot_y_ticks :\n",
    "        plt.yticks(fontsize=12)\n",
    "    else :\n",
    "        plt.yticks([], [])\n",
    "    \n",
    "    if y_min is not None and y_max is not None :\n",
    "        plt.ylim(y_min, y_max)\n",
    "    elif y_min is not None :\n",
    "        plt.ylim(y_min)\n",
    "    else :\n",
    "        plt.ylim(\n",
    "            np.min(importance_scores) - 0.1 * np.max(np.abs(importance_scores)),\n",
    "            np.max(importance_scores) + 0.1 * np.max(np.abs(importance_scores))\n",
    "        )\n",
    "    \n",
    "    plt.axhline(y=0., color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "    #for axis in fig.axes :\n",
    "    #    axis.get_xaxis().set_visible(False)\n",
    "    #    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(fig_name + \".png\", transparent=True, dpi=300)\n",
    "        plt.savefig(fig_name + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_pwm_2(pwm, figsize=(16, 2), plot_y_ticks=True, y_min=None, y_max=None, save_figs=False, fig_name=\"default\") :\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    height_base = 0.\n",
    "    logo_height = 1.0\n",
    "    \n",
    "    for j in range(0, pwm.shape[0]) :\n",
    "        sort_index = np.argsort(pwm[j, :])\n",
    "\n",
    "        for ii in range(0, 4) :\n",
    "            i = sort_index[ii]\n",
    "\n",
    "            nt_prob = pwm[j, i]# * conservation[j]\n",
    "\n",
    "            nt = ''\n",
    "            if i == 0 :\n",
    "                nt = 'A'\n",
    "            elif i == 1 :\n",
    "                nt = 'C'\n",
    "            elif i == 2 :\n",
    "                nt = 'G'\n",
    "            elif i == 3 :\n",
    "                nt = 'T'\n",
    "\n",
    "            color = None\n",
    "            if ii == 0 :\n",
    "                dna_letter_at(nt, j + 0.5, height_base, nt_prob * logo_height, ax, color=color)\n",
    "            else :\n",
    "                prev_prob = np.sum(pwm[j, sort_index[:ii]]) * logo_height # * conservation[j]\n",
    "                dna_letter_at(nt, j + 0.5, height_base + prev_prob, nt_prob * logo_height, ax, color=color)\n",
    "    \n",
    "    plt.sca(ax)\n",
    "    plt.xticks([], [])\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "    \n",
    "    plt.xlim((0, pwm.shape[0]))\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    \n",
    "    if plot_y_ticks :\n",
    "        plt.yticks(fontsize=12)\n",
    "    else :\n",
    "        plt.yticks([], [])\n",
    "    \n",
    "    if y_min is not None and y_max is not None :\n",
    "        plt.ylim(y_min, y_max)\n",
    "    elif y_min is not None :\n",
    "        plt.ylim(y_min)\n",
    "    else :\n",
    "        plt.ylim(\n",
    "            min(0., np.min(np.sum(pwm, axis=-1))) - 0.01 * np.max(np.abs(np.sum(pwm, axis=-1))),\n",
    "            max(0., np.max(np.sum(pwm, axis=-1))) + 0.01 * np.max(np.abs(np.sum(pwm, axis=-1)))\n",
    "        )\n",
    "    \n",
    "    print(np.min(np.sum(pwm, axis=-1)) - 0.1 * np.max(np.abs(np.sum(pwm, axis=-1))))\n",
    "    print(np.max(np.sum(pwm, axis=-1)) + 0.1 * np.max(np.abs(np.sum(pwm, axis=-1))))\n",
    "    \n",
    "    plt.axhline(y=0., color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "    #for axis in fig.axes :\n",
    "    #    axis.get_xaxis().set_visible(False)\n",
    "    #    axis.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_figs :\n",
    "        plt.savefig(fig_name + \".png\", transparent=True, dpi=300)\n",
    "        plt.savefig(fig_name + \".eps\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#One-hot-encoder\n",
    "class SequenceEncoder :\n",
    "    \n",
    "    def __init__(self, encoder_type_id, encode_dims) :\n",
    "        self.encoder_type_id = encoder_type_id\n",
    "        self.encode_dims = encode_dims\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, seq) :\n",
    "        return self.encode(seq)\n",
    "    \n",
    "class OneHotEncoder(SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_length, channel_map) :\n",
    "        super(OneHotEncoder, self).__init__('onehot', (seq_length, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_length\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            val : key for key, val in channel_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            if max_nt == 1 :\n",
    "                seq += self.decode_map[argmax_nt]\n",
    "            else :\n",
    "                seq += self.decode_map[self.n_channels - 1]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        encoding = np.array(encoding_mat[row_index, :].todense()).reshape(-1, 4)\n",
    "        return self.decode(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (5267, 10, 205, 4)\n",
      "m.shape = (5267, 10)\n",
      "l.shape = (5267, 10)\n",
      "c.shape = (5267, 10, 28)\n",
      "y.shape = (5267, 10, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('polyadb_features_pas_3_utr3_perturb.csv', sep='\\t')\n",
    "\n",
    "save_dict = np.load(\"polyadb_features_pas_3_utr3_perturb.npz\")\n",
    "x, m, l, c, y = save_dict['x'], save_dict['m'], save_dict['l'], save_dict['c'], save_dict['y']\n",
    "\n",
    "print(\"x.shape = \" + str(x.shape))\n",
    "print(\"m.shape = \" + str(m.shape))\n",
    "print(\"l.shape = \" + str(l.shape))\n",
    "print(\"c.shape = \" + str(c.shape))\n",
    "print(\"y.shape = \" + str(y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5267, 10)\n"
     ]
    }
   ],
   "source": [
    "#Cache/Load APARENT2 baseline score\n",
    "\n",
    "#np.save(\"polyadb_features_pas_3_utr3_perturb_aparent2_all_scores\", s)\n",
    "s = np.load(\"polyadb_features_pas_3_utr3_perturb_aparent2_all_scores.npy\")\n",
    "\n",
    "print(s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5267, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist_index = np.array([np.nonzero(m[i, :])[0][-1] for i in range(m.shape[0])])\n",
    "\n",
    "dist_mask = np.zeros(m.shape)\n",
    "for i in range(m.shape[0]) :\n",
    "    dist_mask[i, dist_index[i]] = 1.\n",
    "\n",
    "y_dist = []\n",
    "for i in range(y.shape[0]) :\n",
    "    y_dist.append(y[i:i+1, dist_index[i], :])\n",
    "\n",
    "y_dist = np.concatenate(y_dist, axis=0)\n",
    "\n",
    "print(y_dist.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#Load tissue-specific PAS model and generate scores for select tissue types\n",
    "\n",
    "subset_cell_types = np.array([\n",
    "    'NT',\n",
    "    'CPSF4',\n",
    "    'CPSF6',\n",
    "    'CSTF1',\n",
    "    'CSTF3',\n",
    "    'FIP1L1',\n",
    "    'NUDT21',\n",
    "    'RBBP6',\n",
    "    'SRSF3',\n",
    "    'SYMPK',\n",
    "    'THOC5'\n",
    "], dtype=np.object)\n",
    "\n",
    "subset_cell_type_dict = {\n",
    "    cell_type : cell_type_i for cell_type_i, cell_type in enumerate(subset_cell_types)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "#Define tissue-/cell- types\n",
    "\n",
    "cell_types = np.array([\n",
    "    'rpm',\n",
    "    'NT',\n",
    "    'CDC73',\n",
    "    'CPSF1',\n",
    "    'CPSF2',\n",
    "    'CPSF3',\n",
    "    'CPSF3L',\n",
    "    'CPSF4',\n",
    "    'CPSF6',\n",
    "    'CSTF1',\n",
    "    'CSTF3',\n",
    "    'CTR9',\n",
    "    'FIP1L1',\n",
    "    'LEO1',\n",
    "    'NUDT21',\n",
    "    'PABPC1',\n",
    "    'PABPN1',\n",
    "    'PAF1',\n",
    "    'PAPOLA',\n",
    "    'PCF11',\n",
    "    'RBBP6',\n",
    "    'RPRD1A',\n",
    "    'RPRD1B',\n",
    "    'SCAF8',\n",
    "    'SF3A1',\n",
    "    'SRSF3',\n",
    "    'SYMPK',\n",
    "    'THOC5'\n",
    "], dtype=np.object)\n",
    "\n",
    "cell_type_dict = {\n",
    "    cell_type : cell_type_i for cell_type_i, cell_type in enumerate(cell_types)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PAS network definition\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, LocallyConnected2D, MaxPooling2D, GlobalMaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Conv1D, LocallyConnected1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, CuDNNLSTM, CuDNNGRU, BatchNormalization, LocallyConnected2D, Permute, TimeDistributed, Bidirectional\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.layers.merge import _Merge\n",
    "import keras.losses\n",
    "\n",
    "def load_pas_network(n_cell_types=1, n_dil=6, n_channels=32, filter_size=3, filter_size_0=5, nonneg_up_to=0) :\n",
    "    \n",
    "    conv_0 = Conv2D(n_channels, kernel_size=(1, filter_size_0), kernel_constraint=keras.constraints.NonNeg() if nonneg_up_to > 0 else None, padding='same', activation='relu', name='pasnet_conv2d_0')\n",
    "    \n",
    "    drop_0 = Dropout(0.5, name='pasnet_drop_0')\n",
    "    \n",
    "    convs = [\n",
    "        Conv2D(n_channels, kernel_size=(1, filter_size), kernel_constraint=keras.constraints.NonNeg() if i < nonneg_up_to else None, padding='same', activation='relu', dilation_rate=2**i, name='pasnet_conv2d_' + str(i)) for i in range(1, n_dil+1)\n",
    "    ]\n",
    "    \n",
    "    drops = [\n",
    "        Dropout(0.5, name='pasnet_drop_' + str(i)) for i in range(1, n_dil+1)\n",
    "    ]\n",
    "    \n",
    "    adds = [\n",
    "        Lambda(lambda x: x[0] + x[1], name='pasnet_add_' + str(i)) for i in range(1, n_dil+1)\n",
    "    ]\n",
    "    \n",
    "    pool = Lambda(lambda x: K.mean(x, axis=(1, 2)))\n",
    "\n",
    "    final_dense = Dense(n_cell_types*3, activation='linear', kernel_initializer='zeros', bias_initializer='zeros', name='pasnet_dense_2')\n",
    "    final_reshape = Lambda(lambda x: K.reshape(x, (K.shape(x)[0], n_cell_types, 3)))\n",
    "    \n",
    "    def _net_func(sequence_input) :\n",
    "        \n",
    "        x = drop_0(conv_0(sequence_input))\n",
    "\n",
    "        for i in range(1, n_dil+1):\n",
    "            x = adds[i-1]([drops[i-1](convs[i-1](x)), x])\n",
    "\n",
    "        pool_out = pool(x)\n",
    "\n",
    "        final_dense_out = final_dense(pool_out)\n",
    "        \n",
    "        return final_reshape(final_dense_out)\n",
    "\n",
    "    return _net_func\n",
    "\n",
    "def _load_pas_model(model_name, n_cell_types=1) :\n",
    "    \n",
    "    seq_input = Input(shape=(1, 205, 4), name='seq_input')\n",
    "    \n",
    "    pas_net = load_pas_network(n_cell_types=n_cell_types)\n",
    "    \n",
    "    pred_output = pas_net(seq_input)\n",
    "    \n",
    "    pas_model = Model(seq_input, pred_output)\n",
    "    pas_model.load_weights(model_name, by_name=True)\n",
    "    pas_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.SGD(0.1))\n",
    "    \n",
    "    return pas_model\n",
    "\n",
    "def _predict_multi_pas(pas_model, x, batch_size=32) :\n",
    "    \n",
    "    y_preds = []\n",
    "    for k in range(x.shape[1]) :\n",
    "        y_preds.append(pas_model.predict(x=[x[:, k:k+1, ...]], batch_size=32)[:, None, ...])\n",
    "    \n",
    "    return np.concatenate(y_preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "n_bootstraps = 5\n",
    "n_cell_types = subset_cell_types.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 23:32:35.035516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-25 23:32:35.512707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.513115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2023-04-25 23:32:35.513170: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-25 23:32:35.516398: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-25 23:32:35.517876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-25 23:32:35.518218: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-25 23:32:35.521725: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-25 23:32:35.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-04-25 23:32:35.522662: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-04-25 23:32:35.522780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.523195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.523527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
      "2023-04-25 23:32:35.532827: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2023-04-25 23:32:35.534750: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f2a1f34530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-25 23:32:35.534774: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-04-25 23:32:35.617116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.618994: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f2a1f9ae60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-25 23:32:35.619022: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1\n",
      "2023-04-25 23:32:35.619330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.619733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
      "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
      "pciBusID: 0000:00:04.0\n",
      "2023-04-25 23:32:35.619793: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-25 23:32:35.619869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-25 23:32:35.619905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-25 23:32:35.619924: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-25 23:32:35.619949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-25 23:32:35.619975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-04-25 23:32:35.620004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-04-25 23:32:35.620137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.620642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.621064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
      "2023-04-25 23:32:35.621171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-04-25 23:32:35.960414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-25 23:32:35.960467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
      "2023-04-25 23:32:35.960476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
      "2023-04-25 23:32:35.960779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.961444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-25 23:32:35.961908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7057 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nts_ensemble = np.concatenate([_predict_multi_pas(tissue_models[bootstrap_ix], x, batch_size=32)[..., None] for bootstrap_ix in range(n_bootstraps)], axis=-1)\\n\\nts = np.mean(ts_ensemble, axis=-1)\\n\\nprint(\"ts.shape = \" + str(ts.shape))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict tissue model scores\n",
    "\n",
    "tissue_models = [\n",
    "    _load_pas_model(\"saved_models/perturb_resnet_utr3_covar_drop_ensemble_\" + str(bootstrap_ix) + \"_pas_model.h5\", n_cell_types=n_cell_types) for bootstrap_ix in range(n_bootstraps)\n",
    "]\n",
    "'''\n",
    "ts_ensemble = np.concatenate([_predict_multi_pas(tissue_models[bootstrap_ix], x, batch_size=32)[..., None] for bootstrap_ix in range(n_bootstraps)], axis=-1)\n",
    "\n",
    "ts = np.mean(ts_ensemble, axis=-1)\n",
    "\n",
    "print(\"ts.shape = \" + str(ts.shape))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5267, 10, 11, 3, 5)\n",
      "(5267, 10, 11, 3)\n"
     ]
    }
   ],
   "source": [
    "#Cache/Load tissue scores\n",
    "'''\n",
    "np.save(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_ts_ensemble\", ts_ensemble)\n",
    "np.save(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_ts\", ts)\n",
    "'''\n",
    "ts_ensemble = np.load(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_ts_ensemble.npy\")\n",
    "ts = np.load(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_ts.npy\")\n",
    "\n",
    "print(ts_ensemble.shape)\n",
    "print(ts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute normalization statistics\n",
    "\n",
    "cell_type_ixs = [cell_type_dict[ct] for ct in subset_cell_types.tolist()]\n",
    "\n",
    "flat_x = np.reshape(x, (x.shape[0] * x.shape[1], 1, 205, 4))\n",
    "flat_ts_ensemble = np.reshape(ts_ensemble, (x.shape[0] * x.shape[1], n_cell_types, n_bootstraps, 3))\n",
    "flat_ts = np.reshape(ts, (x.shape[0] * x.shape[1], n_cell_types, 3))\n",
    "flat_s = np.reshape(s, (x.shape[0] * x.shape[1],))\n",
    "flat_y = np.reshape(y[:, :, cell_type_ixs], (x.shape[0] * x.shape[1], n_cell_types))\n",
    "flat_gene_ind = np.reshape(np.tile(np.arange(x.shape[0])[:, None], (1, x.shape[1])), (x.shape[0] * x.shape[1],))\n",
    "flat_pas_ind = np.reshape(np.tile(np.arange(x.shape[1])[None, :], (x.shape[0], 1)), (x.shape[0] * x.shape[1],))\n",
    "\n",
    "flat_m = np.reshape(m, (x.shape[0] * x.shape[1],))\n",
    "flat_dist_mask = np.reshape(dist_mask, (x.shape[0] * x.shape[1],))\n",
    "\n",
    "flat_keep_index = np.nonzero(flat_m >= 1)[0]\n",
    "\n",
    "flat_x = flat_x[flat_keep_index, ...]\n",
    "flat_ts_ensemble = flat_ts_ensemble[flat_keep_index, ...]\n",
    "flat_ts = flat_ts[flat_keep_index, ...]\n",
    "flat_s = flat_s[flat_keep_index, ...]\n",
    "flat_y = flat_y[flat_keep_index, ...]\n",
    "flat_gene_ind = flat_gene_ind[flat_keep_index, ...]\n",
    "flat_pas_ind = flat_pas_ind[flat_keep_index, ...]\n",
    "\n",
    "flat_m = flat_m[flat_keep_index, ...]\n",
    "flat_dist_mask = flat_dist_mask[flat_keep_index, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct masks for proximal/middle/distal sites\n",
    "\n",
    "flat_prox_mask = np.array((flat_pas_ind == 0), dtype=np.float32)\n",
    "flat_middle_mask = 1. - flat_dist_mask - flat_prox_mask\n",
    "\n",
    "flat_masks = [\n",
    "    flat_prox_mask,\n",
    "    flat_middle_mask,\n",
    "    flat_dist_mask\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load processed (flattened) PAS IDs from original dataframe\n",
    "\n",
    "flat_ids = np.load(\"polyadb_features_pas_3_utr3_perturb_flat_ids.npy\", allow_pickle=True)\n",
    "\n",
    "#Compile and flatten gene names\n",
    "flat_gene_names = []\n",
    "for _, row in df.iterrows() :\n",
    "    flat_gene_names.extend([row['gene'] for k in range(m.shape[1]) if row['pas_exists_' + str(k)] == 1])\n",
    "\n",
    "flat_gene_names = np.array(flat_gene_names, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_scores.shape = (11, 3, 14764, 1, 205, 4)\n",
      "(bootstrap_ix = 0) flat_scores.shape = (11, 3, 14764, 1, 205, 4)\n",
      "(bootstrap_ix = 1) flat_scores.shape = (11, 3, 14764, 1, 205, 4)\n",
      "(bootstrap_ix = 2) flat_scores.shape = (11, 3, 14764, 1, 205, 4)\n"
     ]
    }
   ],
   "source": [
    "#Re-load gated importance scores\n",
    "\n",
    "flat_scores = np.load(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_flat_g_scores.npy\")\n",
    "flat_scores = np.tile(flat_scores, (1, 1, 1, 1, 1, 4)) * flat_x[None, None, ...]\n",
    "\n",
    "print(\"flat_scores.shape = \" + str(flat_scores.shape))\n",
    "\n",
    "#Re-load gated importance scores (bootstrap replicates)\n",
    "\n",
    "n_bootstraps_ism = 3\n",
    "\n",
    "flat_scores_ensemble = []\n",
    "\n",
    "for bootstrap_ix in range(n_bootstraps_ism) :\n",
    "    flat_scores_curr = np.load(\"polyadb_features_pas_3_utr3_perturb_resnet_covar_drop_flat_g_scores_bootstrap_\" + str(bootstrap_ix) + \".npy\")\n",
    "    flat_scores_curr = np.tile(flat_scores_curr, (1, 1, 1, 1, 1, 4)) * flat_x[None, None, ...]\n",
    "    \n",
    "    flat_scores_ensemble.append(flat_scores_curr)\n",
    "    \n",
    "    print(\"(bootstrap_ix = \" + str(bootstrap_ix) + \") flat_scores.shape = \" + str(flat_scores_curr.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge proximal/middle/distal predictions and ISM scores\n",
    "\n",
    "flat_ts_merged = np.zeros((flat_x.shape[0], n_cell_types))\n",
    "\n",
    "flat_ts_merged[flat_prox_mask == 1., ...] = flat_ts[flat_prox_mask == 1., ..., 0]\n",
    "flat_ts_merged[flat_middle_mask == 1., ...] = flat_ts[flat_middle_mask == 1., ..., 1]\n",
    "flat_ts_merged[flat_dist_mask == 1., ...] = flat_ts[flat_dist_mask == 1., ..., 2]\n",
    "\n",
    "flat_scores_merged = np.zeros((n_cell_types, flat_x.shape[0], 1, 205, 4))\n",
    "\n",
    "flat_scores_merged[:, flat_prox_mask == 1., ...] = flat_scores[:, 0, flat_prox_mask == 1., ...]\n",
    "flat_scores_merged[:, flat_middle_mask == 1., ...] = flat_scores[:, 1, flat_middle_mask == 1., ...]\n",
    "flat_scores_merged[:, flat_dist_mask == 1., ...] = flat_scores[:, 2, flat_dist_mask == 1., ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mononuc_native = [0.3, 0.17, 0.18, 0.35]\n",
      "mononuc_neutral = [0.31, 0.17, 0.17, 0.35]\n",
      "mononuc_weak = [0.28, 0.2, 0.21, 0.31]\n",
      "mononuc_stong = [0.3, 0.15, 0.16, 0.39]\n"
     ]
    }
   ],
   "source": [
    "#Calculate 'native' background nucleotide frequencies\n",
    "s_qtl_lo = np.quantile(flat_s, q=0.0)\n",
    "s_qtl_hi = np.quantile(flat_s, q=1.0)\n",
    "\n",
    "filter_index = np.nonzero((flat_s >= s_qtl_lo) & (flat_s < s_qtl_hi))[0]\n",
    "\n",
    "#Calculate filtered mononucleotide background frequencies\n",
    "mononuc_native = np.sum(flat_x[filter_index, 0, 0:146, :], axis=(0, 1)) / np.sum(flat_x[filter_index, 0, 0:146, :])\n",
    "\n",
    "print(\"mononuc_native = \" + str(np.round(mononuc_native, 2).tolist()))\n",
    "\n",
    "#Calculate 'neutral' background nucleotide frequencies\n",
    "s_qtl_lo = np.quantile(flat_s, q=0.45)\n",
    "s_qtl_hi = np.quantile(flat_s, q=0.55)\n",
    "\n",
    "filter_index = np.nonzero((flat_s >= s_qtl_lo) & (flat_s < s_qtl_hi))[0]\n",
    "\n",
    "#Calculate filtered mononucleotide background frequencies\n",
    "mononuc_neutral = np.sum(flat_x[filter_index, 0, 0:146, :], axis=(0, 1)) / np.sum(flat_x[filter_index, 0, 0:146, :])\n",
    "\n",
    "print(\"mononuc_neutral = \" + str(np.round(mononuc_neutral, 2).tolist()))\n",
    "\n",
    "#Calculate 'weak' background nucleotide frequencies\n",
    "s_qtl_lo = np.quantile(flat_s, q=0.0)\n",
    "s_qtl_hi = np.quantile(flat_s, q=0.1)\n",
    "\n",
    "filter_index = np.nonzero((flat_s >= s_qtl_lo) & (flat_s < s_qtl_hi))[0]\n",
    "\n",
    "#Calculate filtered mononucleotide background frequencies\n",
    "mononuc_weak = np.sum(flat_x[filter_index, 0, 0:146, :], axis=(0, 1)) / np.sum(flat_x[filter_index, 0, 0:146, :])\n",
    "\n",
    "print(\"mononuc_weak = \" + str(np.round(mononuc_weak, 2).tolist()))\n",
    "\n",
    "#Calculate 'stong' background nucleotide frequencies\n",
    "s_qtl_lo = np.quantile(flat_s, q=0.9)\n",
    "s_qtl_hi = np.quantile(flat_s, q=1.0)\n",
    "\n",
    "filter_index = np.nonzero((flat_s >= s_qtl_lo) & (flat_s < s_qtl_hi))[0]\n",
    "\n",
    "#Calculate filtered mononucleotide background frequencies\n",
    "mononuc_stong = np.sum(flat_x[filter_index, 0, 0:146, :], axis=(0, 1)) / np.sum(flat_x[filter_index, 0, 0:146, :])\n",
    "\n",
    "print(\"mononuc_stong = \" + str(np.round(mononuc_stong, 2).tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for sampling extreme sequence examples according to model scores\n",
    "\n",
    "def _sample_sequences_by_score(cell_type_1_ix=0, cell_type_2_ix=1, pred_cell_type_2_ixs=None, score_ix=0, seq_start=0, seq_end=205, n_sequences=10, ts_diff_qtl=0.99, shuffle_region='up', bg_mode_pos='native', bg_mode_neg='native', bg_mode_neutral='native', n_shuffles=5, shuffle_window_size=9, save_name='default') :\n",
    "    \n",
    "    if pred_cell_type_2_ixs is None :\n",
    "        pred_cell_type_2_ixs = [cell_type_2_ix]\n",
    "    \n",
    "    save_name = save_name + \"_cell_type_1_ix_\" + str(cell_type_1_ix) + \"_cell_type_2_ix_\" + str(cell_type_2_ix) + \"_score_ix_\" + str(score_ix) + \"_n_sequences_\" + str(n_sequences) + \"_\" + shuffle_region + \"_window_size_\" + str(window_size)\n",
    "    \n",
    "    if not os.path.exists(\"./samples/\" + save_name.split(\"/\")[0]) :\n",
    "        os.makedirs(\"./samples/\" + save_name.split(\"/\")[0])\n",
    "    \n",
    "    #Get sequence encoder\n",
    "    acgt_encoder = OneHotEncoder(205, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "    \n",
    "    #Construct backgrounds\n",
    "    \n",
    "    #Native mononucleotide background frequencies\n",
    "    mononuc_dicts_native = [\n",
    "        {\n",
    "            'up' : np.zeros(4, dtype='float32'),\n",
    "            'dn' : np.zeros(4, dtype='float32'),\n",
    "        } for score_ixx in range(len(flat_masks))\n",
    "    ]\n",
    "\n",
    "    for i in range(flat_x.shape[0]) :\n",
    "        for score_ixx in range(len(flat_masks)) :\n",
    "            if flat_masks[score_ixx][i] == 1 :\n",
    "                mononuc_dicts_native[score_ixx]['up'] += np.sum(flat_x[i, 0, 0:70, :], axis=0)\n",
    "                mononuc_dicts_native[score_ixx]['dn'] += np.sum(flat_x[i, 0, 76:146, :], axis=0)\n",
    "\n",
    "    for score_ixx in range(len(flat_masks)) :\n",
    "        mononuc_dicts_native[score_ixx]['up'] /= np.sum(mononuc_dicts_native[score_ixx]['up'])\n",
    "        mononuc_dicts_native[score_ixx]['dn'] /= np.sum(mononuc_dicts_native[score_ixx]['dn'])\n",
    "    \n",
    "    #Uniform frequencies\n",
    "    mononuc_dicts_unif = [\n",
    "        {\n",
    "            'up' : 0.25 * np.ones(4, dtype='float32'),\n",
    "            'dn' : 0.25 * np.ones(4, dtype='float32'),\n",
    "        } for score_ixx in range(len(flat_masks))\n",
    "    ]\n",
    "    \n",
    "    #GT-depleted frequencies\n",
    "    mononuc_dicts_less_gt = [\n",
    "        {\n",
    "            'up' : np.array([0.30, 0.30, 0.25, 0.15], dtype='float32'),\n",
    "            'dn' : np.array([0.30, 0.30, 0.25, 0.15], dtype='float32'),\n",
    "        } for score_ixx in range(len(flat_masks))\n",
    "    ]\n",
    "    \n",
    "    #GT-enriched frequencies\n",
    "    mononuc_dicts_more_gt = [\n",
    "        {\n",
    "            'up' : np.array([0.20, 0.15, 0.25, 0.40], dtype='float32'),\n",
    "            'dn' : np.array([0.20, 0.15, 0.25, 0.40], dtype='float32'),\n",
    "        } for score_ixx in range(len(flat_masks))\n",
    "    ]\n",
    "\n",
    "    #Pos frequencies\n",
    "    bg_dict_pos = None\n",
    "    if bg_mode_pos == 'native' :\n",
    "        bg_dict_pos = mononuc_dicts_native[score_ix]\n",
    "    elif bg_mode_pos == 'unif' :\n",
    "        bg_dict_pos = mononuc_dicts_unif[score_ix]\n",
    "    elif bg_mode_pos == 'less_gt' :\n",
    "        bg_dict_pos = mononuc_dicts_less_gt[score_ix]\n",
    "    elif bg_mode_pos == 'more_gt' :\n",
    "        bg_dict_pos = mononuc_dicts_more_gt[score_ix]\n",
    "\n",
    "    #Neg frequencies\n",
    "    bg_dict_neg = None\n",
    "    if bg_mode_neg == 'native' :\n",
    "        bg_dict_neg = mononuc_dicts_native[score_ix]\n",
    "    elif bg_mode_neg == 'unif' :\n",
    "        bg_dict_neg = mononuc_dicts_unif[score_ix]\n",
    "    elif bg_mode_neg == 'less_gt' :\n",
    "        bg_dict_neg = mononuc_dicts_less_gt[score_ix]\n",
    "    elif bg_mode_neg == 'more_gt' :\n",
    "        bg_dict_neg = mononuc_dicts_more_gt[score_ix]\n",
    "\n",
    "    #Neutral frequencies\n",
    "    bg_dict_neutral = None\n",
    "    if bg_mode_neutral == 'native' :\n",
    "        bg_dict_neutral = mononuc_dicts_native[score_ix]\n",
    "    elif bg_mode_neutral == 'unif' :\n",
    "        bg_dict_neutral = mononuc_dicts_unif[score_ix]\n",
    "    elif bg_mode_neutral == 'less_gt' :\n",
    "        bg_dict_neutral = mononuc_dicts_less_gt[score_ix]\n",
    "    elif bg_mode_neutral == 'more_gt' :\n",
    "        bg_dict_neutral = mononuc_dicts_more_gt[score_ix]\n",
    "    \n",
    "    #Get top N most extremely perturbed PASs (model scores, measurements and ISMs)\n",
    "    flat_ts_diff = flat_ts[:, cell_type_2_ix, score_ix] - flat_ts[:, cell_type_1_ix, score_ix]\n",
    "    flat_y_diff = flat_y[:, cell_type_2_ix] - flat_y[:, cell_type_1_ix]\n",
    "    \n",
    "    flat_diff_scores = np.sum((flat_scores[cell_type_1_ix, score_ix, :, 0, seq_start:seq_end, :] - flat_scores[cell_type_2_ix, score_ix, :, 0, seq_start:seq_end, :]) * flat_x[:, 0, seq_start:seq_end, :], axis=-1)\n",
    "    \n",
    "    #Sorted in ascending order of model scores (negative)\n",
    "    min_ts_diff = np.quantile(flat_ts_diff[flat_masks[score_ix] == 1.], q=1.-ts_diff_qtl)\n",
    "\n",
    "    outlier_index = np.nonzero((flat_ts_diff < min_ts_diff) & (flat_masks[score_ix] == 1.))[0]\n",
    "    \n",
    "    outlier_ts_diff = flat_ts_diff[outlier_index]\n",
    "    outlier_y_diff = flat_y_diff[outlier_index]\n",
    "\n",
    "    outlier_index = outlier_index[np.argsort(outlier_y_diff)][:n_sequences].tolist()\n",
    "\n",
    "    #Save sequences to file\n",
    "    with open(\"./samples/\" + save_name + \"_pos.txt\", \"w\") as f :\n",
    "        \n",
    "        diff_score_col_str = \"\"\n",
    "        for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "            for sc_ix in range(3) :\n",
    "                diff_score_col_str += \"\\tdiff_ct_\" + str(pred_cell_type_2_ix) + \"_score_\" + str(sc_ix)\n",
    "        \n",
    "        f.write(\"gene_id\\tpas_id\\texperiment\\tseq\\tusage_nt\\tusage_perturb\\tusage_diff\" + diff_score_col_str + \"\\n\")\n",
    "        \n",
    "        for i in outlier_index :\n",
    "            gene_id = flat_gene_names[i]\n",
    "            pas_id = flat_ids[i]\n",
    "            experiment = 'wt'\n",
    "            wt_seq = acgt_encoder.decode(flat_x[i, 0, seq_start:seq_end, :])\n",
    "            \n",
    "            y_curr_nt = round(flat_y[i, cell_type_1_ix], 4)\n",
    "            y_curr_perturb = round(flat_y[i, cell_type_2_ix], 4)\n",
    "            \n",
    "            y_curr_diff = round(-flat_y_diff[i], 4)\n",
    "            \n",
    "            pred_curr = []\n",
    "            for bootstrap_ix in range(n_bootstraps) :\n",
    "                pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                    flat_x[i:i+1, ...]\n",
    "                ], batch_size=1, verbose=False))\n",
    "            \n",
    "            pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "            \n",
    "            diff_score_str = \"\"\n",
    "            for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                for sc_ix in range(3) :\n",
    "                    diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "            \n",
    "            #Store wildtype sequence\n",
    "            f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + experiment + \"\\t\" + wt_seq + \"\\t\" + str(y_curr_nt) + \"\\t\" + str(y_curr_perturb) + \"\\t\" + str(y_curr_diff) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            diff_scores_curr = np.copy(flat_diff_scores[i, :])\n",
    "            \n",
    "            #Get mode position of importance in upstream region\n",
    "            up_argmax_1 = np.argmax(diff_scores_curr[shuffle_window_size // 2:70 - shuffle_window_size // 2]) + shuffle_window_size // 2\n",
    "            \n",
    "            #Mark covered positions\n",
    "            diff_scores_curr[max(up_argmax_1 - shuffle_window_size + 1, 0): min(up_argmax_1 + shuffle_window_size, diff_scores_curr.shape[0])] = -1e6\n",
    "            \n",
    "            #Select second mode\n",
    "            up_argmax_2 = np.argmax(diff_scores_curr[shuffle_window_size // 2:70 - shuffle_window_size // 2]) + shuffle_window_size // 2\n",
    "            \n",
    "            #Get mode position of importance in downstream region\n",
    "            dn_argmax_1 = np.argmax(diff_scores_curr[76 + shuffle_window_size // 2:146 - shuffle_window_size // 2]) + 76 + shuffle_window_size // 2\n",
    "            \n",
    "            #Mark covered positions\n",
    "            diff_scores_curr[max(dn_argmax_1 - shuffle_window_size + 1, 0): min(dn_argmax_1 + shuffle_window_size, diff_scores_curr.shape[0])] = -1e6\n",
    "            \n",
    "            #Select second mode\n",
    "            dn_argmax_2 = np.argmax(diff_scores_curr[76 + shuffle_window_size // 2:146 - shuffle_window_size // 2]) + 76 + shuffle_window_size // 2\n",
    "            \n",
    "            argmax_pos_1, argmax_pos_2 = -1, -1\n",
    "            if shuffle_region == 'up' :\n",
    "                argmax_pos_1, argmax_pos_2 = up_argmax_1, up_argmax_2\n",
    "            elif shuffle_region == 'dn' or shuffle_region == 'cse' :\n",
    "                argmax_pos_1, argmax_pos_2 = dn_argmax_1, dn_argmax_2\n",
    "            \n",
    "            bg_nuc = bg_dict_pos[shuffle_region] if shuffle_region in ['up', 'dn'] else None\n",
    "            \n",
    "            shuffled_seqs_1, shuffled_seqs_2 = None, None\n",
    "            if shuffle_region in ['up', 'dn'] :\n",
    "                shuffled_seqs_1 = _shuffle_seq(wt_seq, [[argmax_pos_1 - shuffle_window_size // 2, argmax_pos_1 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "                shuffled_seqs_2 = _shuffle_seq(wt_seq, [[argmax_pos_2 - shuffle_window_size // 2, argmax_pos_2 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "            else :\n",
    "                flank_len = ((shuffle_window_size + 1) - 6) // 2\n",
    "                \n",
    "                shuffled_seqs_1 = _shuffle_cse(wt_seq, [[70-flank_len, 76+flank_len]], n_samples=n_shuffles)\n",
    "                \n",
    "                if argmax_pos_1 - shuffle_window_size // 2 <= 76+flank_len-1 :\n",
    "                    argmax_pos_1 += (76+flank_len) - (argmax_pos_1 - shuffle_window_size // 2)\n",
    "                \n",
    "                shuffled_seqs_2 = _shuffle_seq(wt_seq, [[argmax_pos_1 - shuffle_window_size // 2, argmax_pos_1 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "            \n",
    "            #Construct sequences where both regions are shuffled simultaneously\n",
    "            shuffled_seqs_1_and_2 = []\n",
    "            for shuffled_seq_1, shuffled_seq_2 in zip(shuffled_seqs_1, shuffled_seqs_2) :\n",
    "                if shuffle_region in ['up', 'dn'] :\n",
    "                    if argmax_pos_1 < argmax_pos_2 :\n",
    "                        shuffled_seq_1_and_2 = wt_seq[:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_1[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:argmax_pos_2 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_2 - shuffle_window_size // 2:argmax_pos_2 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_2 + shuffle_window_size // 2 + 1:]\n",
    "                    else :\n",
    "                        shuffled_seq_1_and_2 = wt_seq[:argmax_pos_2 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_2 - shuffle_window_size // 2:argmax_pos_2 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_2 + shuffle_window_size // 2 + 1:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_1[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:]\n",
    "                else : #CSE\n",
    "                    shuffled_seq_1_and_2 = wt_seq[:70] + shuffled_seq_1[70:76] + wt_seq[76:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:]\n",
    "                \n",
    "                shuffled_seqs_1_and_2.append(shuffled_seq_1_and_2)\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_1)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_1[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_1_pos_\" + str(argmax_pos_1) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_2)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_2[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_2_pos_\" + str(argmax_pos_2) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_1_and_2)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_1_and_2[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_1_and_2_pos_\" + str(argmax_pos_1) + \"_and_\" + str(argmax_pos_2) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "\n",
    "    #Sorted in descending order of model scores (positive)\n",
    "    min_ts_diff = np.quantile(flat_ts_diff[flat_masks[score_ix] == 1.], q=ts_diff_qtl)\n",
    "\n",
    "    outlier_index = np.nonzero((flat_ts_diff > min_ts_diff) & (flat_masks[score_ix] == 1.))[0]\n",
    "\n",
    "    outlier_ts_diff = flat_ts_diff[outlier_index]\n",
    "    outlier_y_diff = flat_y_diff[outlier_index]\n",
    "\n",
    "    outlier_index = outlier_index[np.argsort(outlier_y_diff)[::-1]][:n_sequences].tolist()\n",
    "\n",
    "    #Save sequences to file\n",
    "    with open(\"./samples/\" + save_name + \"_neg.txt\", \"w\") as f :\n",
    "        \n",
    "        diff_score_col_str = \"\"\n",
    "        for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "            for sc_ix in range(3) :\n",
    "                diff_score_col_str += \"\\tdiff_ct_\" + str(pred_cell_type_2_ix) + \"_score_\" + str(sc_ix)\n",
    "        \n",
    "        f.write(\"gene_id\\tpas_id\\texperiment\\tseq\\tusage_nt\\tusage_perturb\\tusage_diff\" + diff_score_col_str + \"\\n\")\n",
    "        \n",
    "        for i in outlier_index :\n",
    "            gene_id = flat_gene_names[i]\n",
    "            pas_id = flat_ids[i]\n",
    "            experiment = 'wt'\n",
    "            wt_seq = acgt_encoder.decode(flat_x[i, 0, seq_start:seq_end, :])\n",
    "            \n",
    "            y_curr_nt = round(flat_y[i, cell_type_1_ix], 4)\n",
    "            y_curr_perturb = round(flat_y[i, cell_type_2_ix], 4)\n",
    "            \n",
    "            y_curr_diff = round(-flat_y_diff[i], 4)\n",
    "            \n",
    "            pred_curr = []\n",
    "            for bootstrap_ix in range(n_bootstraps) :\n",
    "                pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                    flat_x[i:i+1, ...]\n",
    "                ], batch_size=1, verbose=False))\n",
    "            \n",
    "            pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "            \n",
    "            diff_score_str = \"\"\n",
    "            for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                for sc_ix in range(3) :\n",
    "                    diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "            \n",
    "            #Store wildtype sequence\n",
    "            f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + experiment + \"\\t\" + wt_seq + \"\\t\" + str(y_curr_nt) + \"\\t\" + str(y_curr_perturb) + \"\\t\" + str(y_curr_diff) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            diff_scores_curr = np.copy(flat_diff_scores[i, :])\n",
    "            \n",
    "            #Get mode position of importance in upstream region\n",
    "            up_argmax_1 = np.argmin(diff_scores_curr[shuffle_window_size // 2:70 - shuffle_window_size // 2]) + shuffle_window_size // 2\n",
    "            \n",
    "            #Mark covered positions\n",
    "            diff_scores_curr[max(up_argmax_1 - shuffle_window_size + 1, 0): min(up_argmax_1 + shuffle_window_size, diff_scores_curr.shape[0])] = 1e6\n",
    "            \n",
    "            #Select second mode\n",
    "            up_argmax_2 = np.argmin(diff_scores_curr[shuffle_window_size // 2:70 - shuffle_window_size // 2]) + shuffle_window_size // 2\n",
    "            \n",
    "            #Get mode position of importance in downstream region\n",
    "            dn_argmax_1 = np.argmin(diff_scores_curr[76 + shuffle_window_size // 2:146 - shuffle_window_size // 2]) + 76 + shuffle_window_size // 2\n",
    "            \n",
    "            #Mark covered positions\n",
    "            diff_scores_curr[max(dn_argmax_1 - shuffle_window_size + 1, 0): min(dn_argmax_1 + shuffle_window_size, diff_scores_curr.shape[0])] = 1e6\n",
    "            \n",
    "            #Select second mode\n",
    "            dn_argmax_2 = np.argmin(diff_scores_curr[76 + shuffle_window_size // 2:146 - shuffle_window_size // 2]) + 76 + shuffle_window_size // 2\n",
    "            \n",
    "            argmax_pos_1, argmax_pos_2 = -1, -1\n",
    "            if shuffle_region == 'up' :\n",
    "                argmax_pos_1, argmax_pos_2 = up_argmax_1, up_argmax_2\n",
    "            elif shuffle_region == 'dn' or shuffle_region == 'cse' :\n",
    "                argmax_pos_1, argmax_pos_2 = dn_argmax_1, dn_argmax_2\n",
    "            \n",
    "            bg_nuc = bg_dict_neg[shuffle_region] if shuffle_region in ['up', 'dn'] else None\n",
    "            \n",
    "            shuffled_seqs_1, shuffled_seqs_2 = None, None\n",
    "            if shuffle_region in ['up', 'dn'] :\n",
    "                shuffled_seqs_1 = _shuffle_seq(wt_seq, [[argmax_pos_1 - shuffle_window_size // 2, argmax_pos_1 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "                shuffled_seqs_2 = _shuffle_seq(wt_seq, [[argmax_pos_2 - shuffle_window_size // 2, argmax_pos_2 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "            else :\n",
    "                flank_len = ((shuffle_window_size + 1) - 6) // 2\n",
    "                \n",
    "                shuffled_seqs_1 = _shuffle_cse(wt_seq, [[70-flank_len, 76+flank_len]], n_samples=n_shuffles)\n",
    "                \n",
    "                if argmax_pos_1 - shuffle_window_size // 2 <= 76+flank_len-1 :\n",
    "                    argmax_pos_1 += (76+flank_len) - (argmax_pos_1 - shuffle_window_size // 2)\n",
    "                \n",
    "                shuffled_seqs_2 = _shuffle_seq(wt_seq, [[argmax_pos_1 - shuffle_window_size // 2, argmax_pos_1 + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "            \n",
    "            #Construct sequences where both regions are shuffled simultaneously\n",
    "            shuffled_seqs_1_and_2 = []\n",
    "            for shuffled_seq_1, shuffled_seq_2 in zip(shuffled_seqs_1, shuffled_seqs_2) :\n",
    "                if shuffle_region in ['up', 'dn'] :\n",
    "                    if argmax_pos_1 < argmax_pos_2 :\n",
    "                        shuffled_seq_1_and_2 = wt_seq[:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_1[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:argmax_pos_2 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_2 - shuffle_window_size // 2:argmax_pos_2 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_2 + shuffle_window_size // 2 + 1:]\n",
    "                    else :\n",
    "                        shuffled_seq_1_and_2 = wt_seq[:argmax_pos_2 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_2 - shuffle_window_size // 2:argmax_pos_2 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_2 + shuffle_window_size // 2 + 1:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_1[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:]\n",
    "                else : #CSE\n",
    "                    shuffled_seq_1_and_2 = wt_seq[:70] + shuffled_seq_1[70:76] + wt_seq[76:argmax_pos_1 - shuffle_window_size // 2] + shuffled_seq_2[argmax_pos_1 - shuffle_window_size // 2:argmax_pos_1 + shuffle_window_size // 2 + 1] + wt_seq[argmax_pos_1 + shuffle_window_size // 2 + 1:]\n",
    "                \n",
    "                shuffled_seqs_1_and_2.append(shuffled_seq_1_and_2)\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_1)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_1[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_1_pos_\" + str(argmax_pos_1) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_2)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_2[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_2_pos_\" + str(argmax_pos_2) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs_1_and_2)) :\n",
    "                \n",
    "                shuffled_seq = shuffled_seqs_1_and_2[shuffle_i]\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seq)[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_1_and_2_pos_\" + str(argmax_pos_1) + \"_and_\" + str(argmax_pos_2) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seq + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "\n",
    "    #Sorted in descending order of model scores (neutral)\n",
    "    target_ts_diff = np.quantile(flat_ts_diff[flat_masks[score_ix] == 1.], q=0.5)\n",
    "\n",
    "    outlier_index = np.nonzero(flat_masks[score_ix] == 1.)[0]\n",
    "\n",
    "    outlier_ts_diff = flat_ts_diff[outlier_index]\n",
    "\n",
    "    outlier_index = outlier_index[np.argsort((outlier_ts_diff - target_ts_diff)**2)][:n_sequences].tolist()\n",
    "\n",
    "    #Save sequences to file\n",
    "    with open(\"./samples/\" + save_name + \"_neutral.txt\", \"w\") as f :\n",
    "        \n",
    "        diff_score_col_str = \"\"\n",
    "        for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "            for sc_ix in range(3) :\n",
    "                diff_score_col_str += \"\\tdiff_ct_\" + str(pred_cell_type_2_ix) + \"_score_\" + str(sc_ix)\n",
    "        \n",
    "        f.write(\"gene_id\\tpas_id\\texperiment\\tseq\\tusage_nt\\tusage_perturb\\tusage_diff\" + diff_score_col_str + \"\\n\")\n",
    "        \n",
    "        for i in outlier_index :\n",
    "            gene_id = flat_gene_names[i]\n",
    "            pas_id = flat_ids[i]\n",
    "            experiment = 'wt'\n",
    "            wt_seq = acgt_encoder.decode(flat_x[i, 0, seq_start:seq_end, :])\n",
    "            \n",
    "            y_curr_nt = round(flat_y[i, cell_type_1_ix], 4)\n",
    "            y_curr_perturb = round(flat_y[i, cell_type_2_ix], 4)\n",
    "            \n",
    "            y_curr_diff = round(-flat_y_diff[i], 4)\n",
    "            \n",
    "            pred_curr = []\n",
    "            for bootstrap_ix in range(n_bootstraps) :\n",
    "                pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                    flat_x[i:i+1, ...]\n",
    "                ], batch_size=1, verbose=False))\n",
    "            \n",
    "            pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "            \n",
    "            diff_score_str = \"\"\n",
    "            for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                for sc_ix in range(3) :\n",
    "                    diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "            \n",
    "            #Store wildtype sequence\n",
    "            f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + experiment + \"\\t\" + wt_seq + \"\\t\" + str(y_curr_nt) + \"\\t\" + str(y_curr_perturb) + \"\\t\" + str(y_curr_diff) + diff_score_str + \"\\n\")\n",
    "            \n",
    "            #Get mode position of importance in upstream region\n",
    "            up_argmax = np.argmin(np.abs(0. - flat_diff_scores[i, shuffle_window_size // 2:70 - shuffle_window_size // 2])) + shuffle_window_size // 2\n",
    "            \n",
    "            #Get mode position of importance in downstream region\n",
    "            dn_argmax = np.argmin(np.abs(0. - flat_diff_scores[i, 76 + shuffle_window_size // 2:146 - shuffle_window_size // 2])) + 76 + shuffle_window_size // 2\n",
    "            \n",
    "            argmax_pos = -1\n",
    "            if shuffle_region == 'up' :\n",
    "                argmax_pos = up_argmax\n",
    "            elif shuffle_region == 'dn' :\n",
    "                argmax_pos = dn_argmax\n",
    "            \n",
    "            bg_nuc = bg_dict_neutral[shuffle_region] if shuffle_region in ['up', 'dn'] else None\n",
    "            \n",
    "            shuffled_seqs = None\n",
    "            if shuffle_region in ['up', 'dn'] :\n",
    "                shuffled_seqs = _shuffle_seq(wt_seq, [[argmax_pos - shuffle_window_size // 2, argmax_pos + shuffle_window_size // 2 + 1]], bg_nuc, n_samples=n_shuffles, window_size=shuffle_window_size)\n",
    "            else :\n",
    "                flank_len = ((shuffle_window_size + 1) - 6) // 2\n",
    "                shuffled_seqs = _shuffle_cse(wt_seq, [[70 - flank_len, 76 + flank_len]], n_samples=n_shuffles)\n",
    "            \n",
    "            for shuffle_i in range(len(shuffled_seqs)) :\n",
    "                \n",
    "                flat_x_shuffled = acgt_encoder.encode(shuffled_seqs[shuffle_i])[None, None, ...]\n",
    "                \n",
    "                pred_curr = []\n",
    "                for bootstrap_ix in range(n_bootstraps) :\n",
    "                    pred_curr.append(tissue_models[bootstrap_ix].predict(x=[\n",
    "                        flat_x_shuffled\n",
    "                    ], batch_size=1, verbose=False))\n",
    "\n",
    "                pred_curr = np.mean(np.concatenate(pred_curr, axis=0), axis=0)\n",
    "\n",
    "                diff_score_str = \"\"\n",
    "                for pred_cell_type_2_ix in pred_cell_type_2_ixs :\n",
    "                    for sc_ix in range(3) :\n",
    "                        diff_score_str += \"\\t\" + str(round(pred_curr[cell_type_1_ix, sc_ix] - pred_curr[pred_cell_type_2_ix, sc_ix], 4))\n",
    "\n",
    "                #Store shuffled sequence\n",
    "                f.write(gene_id + \"\\t\" + pas_id + \"\\t\" + \"shuffle_1_pos_\" + str(argmax_pos) + \"_repeat\" + str(shuffle_i) + \"\\t\" + shuffled_seqs[shuffle_i] + \"\\t\" + str(0.) + \"\\t\" + str(0.) + \"\\t\" + str(0.) + diff_score_str + \"\\n\")\n",
    "\n",
    "def _shuffle_seq(seq, ablate_regions, bg, n_samples=5, window_size=9) :\n",
    "    \n",
    "    nts = ['A', 'C', 'G', 'T']\n",
    "    \n",
    "    seqs_shuffled = []\n",
    "    \n",
    "    for [ablate_start, ablate_end] in ablate_regions :\n",
    "        \n",
    "        for sample_ix in range(n_samples) :\n",
    "            rand_seq = \"\".join(np.random.choice(nts, size=(window_size,), p=bg, replace=True).tolist())\n",
    "            ablated_seq = seq[:ablate_start] + rand_seq + seq[ablate_end:]\n",
    "\n",
    "            seqs_shuffled.append(ablated_seq)\n",
    "\n",
    "    return seqs_shuffled\n",
    "\n",
    "def _shuffle_cse(seq, ablate_regions, n_samples=5) :\n",
    "    \n",
    "    nts = ['A', 'C', 'G', 'T']\n",
    "    \n",
    "    cses = [\n",
    "        'TATAAA',\n",
    "        'GATAAA',\n",
    "        'CATAAA',\n",
    "        'AGTAAA',\n",
    "        'ACTAAA',\n",
    "        'AATATA',\n",
    "        'AATACA',\n",
    "        'AATAGA',\n",
    "        'AATAAT',\n",
    "        'AATAAC',\n",
    "    ]\n",
    "    \n",
    "    seqs_shuffled = []\n",
    "    \n",
    "    for [ablate_start, ablate_end] in ablate_regions :\n",
    "        \n",
    "        flank_len = ((ablate_end - ablate_start) - 6) // 2\n",
    "        \n",
    "        for sample_ix in range(n_samples) :\n",
    "            \n",
    "            rand_f1 = \"\".join(np.random.choice(nts, size=(flank_len,), p=[0.1, 0.35, 0.35, 0.2], replace=True).tolist())\n",
    "            rand_f2 = \"\".join(np.random.choice(nts, size=(flank_len,), p=[0.1, 0.35, 0.35, 0.2], replace=True).tolist())\n",
    "            \n",
    "            rand_seq = rand_f1 + np.random.choice(cses) + rand_f2\n",
    "            ablated_seq = seq[:ablate_start] + rand_seq + seq[ablate_end:]\n",
    "\n",
    "            seqs_shuffled.append(ablated_seq)\n",
    "\n",
    "    return seqs_shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- window_size = 9 --\n",
      "-- score_ix = 0 --\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 23:33:25.384110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\n",
      "2023-04-25 23:33:25.621789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "-- score_ix = 2 --\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "-- window_size = 11 --\n",
      "-- score_ix = 0 --\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "-- score_ix = 2 --\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n",
      "cell_type_1_ix = NT vs. cell_type_2_ix = NUDT21\n"
     ]
    }
   ],
   "source": [
    "#Store extreme sequence samples for each perturbation based on the model scores (unif background)\n",
    "\n",
    "#Parameter configuration\n",
    "n_sequences = 100\n",
    "\n",
    "experiment_prefix = 'unif_bg_'\n",
    "\n",
    "window_sizes = [9, 11]\n",
    "score_ixs = [0, 2]\n",
    "\n",
    "pred_cell_type_2_ixs = [4, 6, 7, 10]\n",
    "\n",
    "for window_size in window_sizes :\n",
    "    print(\"-- window_size = \" + str(window_size) + \" --\")\n",
    "    \n",
    "    for score_ix in score_ixs :\n",
    "        print(\"-- score_ix = \" + str(score_ix) + \" --\")\n",
    "        \n",
    "        save_name = experiment_prefix + 'window_size_' + str(window_size) + '_score_ix_' + str(score_ix) + '/apa_perturb_v3'\n",
    "\n",
    "        cell_type_index_pairs = [\n",
    "            [0, 4],\n",
    "            [0, 6],\n",
    "            [0, 6],\n",
    "            [0, 7],\n",
    "            [0, 10],\n",
    "        ]\n",
    "\n",
    "        shuffle_regions = [\n",
    "            'dn',\n",
    "            'up',\n",
    "            'dn',\n",
    "            'cse',\n",
    "            'dn',\n",
    "        ]\n",
    "        \n",
    "        bg_modes = [\n",
    "            ['unif', 'unif', 'unif'],\n",
    "            ['unif', 'unif', 'unif'],\n",
    "            ['unif', 'unif', 'unif'],\n",
    "            ['unif', 'unif', 'unif'],\n",
    "            ['unif', 'unif', 'unif'],\n",
    "        ]\n",
    "\n",
    "        for pair_ix, [cell_type_1_ix, cell_type_2_ix] in enumerate(cell_type_index_pairs) :\n",
    "\n",
    "            print(\"cell_type_1_ix = \" + str(subset_cell_types[cell_type_1_ix]) + \" vs. cell_type_2_ix = \" + str(subset_cell_types[cell_type_2_ix]))\n",
    "\n",
    "            _sample_sequences_by_score(\n",
    "                cell_type_1_ix=cell_type_1_ix,\n",
    "                cell_type_2_ix=cell_type_2_ix,\n",
    "                pred_cell_type_2_ixs=pred_cell_type_2_ixs,\n",
    "                score_ix=score_ix,\n",
    "                n_sequences=n_sequences,\n",
    "                ts_diff_qtl=0.96,\n",
    "                shuffle_region=shuffle_regions[pair_ix],\n",
    "                bg_mode_pos=bg_modes[pair_ix][0],\n",
    "                bg_mode_neg=bg_modes[pair_ix][1],\n",
    "                bg_mode_neutral=bg_modes[pair_ix][2],\n",
    "                n_shuffles=5,\n",
    "                shuffle_window_size=window_size,\n",
    "                save_name=save_name,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
